"""
æµå¼æ¨èå¼•æ“æ ¸å¿ƒç»„ä»¶ - ç¬¬ä¸€éƒ¨åˆ†
å–ä»£åŸæœ‰çš„EnhancedAnalyzerï¼Œæ”¯æŒæµå¼å¤„ç†å’Œä»»åŠ¡ç®¡ç†
"""
import asyncio
import json
import uuid
import time
import re
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, AsyncGenerator
import logging

# å¯¼å…¥ç›¸å…³ç»„ä»¶
from .streaming_ai_router import StreamingAIRouter, StreamingAIRequest, progress_manager
from .analyzer import basic_analysis
from .data_fetcher import DataFetcher
from .confidence_fusion import extract_confidence_and_fusion
from .analysis_logger import AnalysisLogger, AnalysisLogViewer

# å¯¼å…¥æ•°æ®åº“æ¨¡å‹
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from backend.models.streaming_models import (
    RecommendationTask, RecommendationResult, TaskProgress, SessionLocal, now_bj
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StreamingRecommendationEngine:
    """æµå¼æ¨èå¼•æ“ - ç³»ç»Ÿæ ¸å¿ƒï¼ˆå–ä»£åŸæœ‰çš„EnhancedAnalyzerï¼‰"""
    
    def __init__(self):
        self.ai_router = StreamingAIRouter()
        self.data_fetcher = DataFetcher()
        self.progress_manager = progress_manager
        
        # é»˜è®¤æƒé‡é…ç½®
        self.default_weights = {
            "technical": 0.6,
            "ai_confidence": 0.4,
            "macro_sentiment": 0.35,
            "news_events": 0.25
        }
    
    async def execute_recommendation_task(self, task_id: str):
        """æ‰§è¡Œæ¨èä»»åŠ¡çš„ä¸»æ–¹æ³•"""
        # åˆå§‹åŒ–åˆ†ææ—¥å¿—è®°å½•å™¨
        analysis_logger = AnalysisLogger(task_id)
        
        # æ³¨å†Œè¿›åº¦å›è°ƒï¼Œç¡®ä¿è¿›åº¦èƒ½æ­£ç¡®ä¿å­˜åˆ°æ•°æ®åº“
        async def progress_callback(progress_data: Dict[str, Any]):
            """è¿›åº¦å›è°ƒå‡½æ•°ï¼Œå°†è¿›åº¦ä¿å­˜åˆ°æ•°æ®åº“"""
            try:
                from .sse_manager import sse_manager
                event_type = progress_data.get('type', 'progress')
                await sse_manager.broadcast_to_task(task_id, event_type, progress_data)
                logger.debug(f"âœ… è¿›åº¦å›è°ƒæˆåŠŸ: {event_type} - {progress_data.get('symbol', '')}")
            except Exception as e:
                logger.error(f"âŒ è¿›åº¦å›è°ƒå¤±è´¥: {e}")
        
        # æ³¨å†Œå›è°ƒ
        self.progress_manager.register_callback(task_id, progress_callback)
        
        task = await self._get_task(task_id)
        if not task:
            raise ValueError(f"Task {task_id} not found")
            
        try:
            await self._update_task_status(task_id, 'running', started_at=now_bj())
            
            # è®°å½•ä»»åŠ¡å¼€å§‹
            request_params = json.loads(task.request_params or '{}')
            analysis_logger.log_task_start({
                'task_type': task.task_type,
                'request_params': request_params,
                'ai_config': json.loads(task.ai_config or '{}'),
                'filter_config': json.loads(task.filter_config or '{}'),
                'weights_config': json.loads(task.weights_config or '{}')
            })
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
            if task.task_type == 'ai':
                await self._process_ai_recommendation(task, analysis_logger)
            elif task.task_type == 'keyword':
                await self._process_keyword_recommendation(task, analysis_logger)
            elif task.task_type == 'market':
                await self._process_market_recommendation(task, analysis_logger)
            elif task.task_type == 'watchlist_batch':
                await self._process_watchlist_batch(task, analysis_logger)
            elif task.task_type == 'watchlist_reanalyze':
                await self._process_watchlist_reanalyze(task, analysis_logger)
            else:
                analysis_logger.log_error('unknown_task_type', f'Unknown task type: {task.task_type}')
                raise ValueError(f"Unknown task type: {task.task_type}")
                
            await self._finalize_task(task_id)
            logger.info(f"âœ… ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            analysis_logger.log_error('task_execution_error', str(e))
            await self._handle_task_error(task_id, str(e))
            logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
            raise
        finally:
            # ä»»åŠ¡ç»“æŸæ—¶æ³¨é”€å›è°ƒ
            self.progress_manager.unregister_callback(task_id)
    
    async def _process_ai_recommendation(self, task: RecommendationTask, analysis_logger: AnalysisLogger):
        """å¤„ç†AIæ¨èä»»åŠ¡ï¼ˆæ‰‹åŠ¨è¾“å…¥è‚¡ç¥¨ä»£ç ï¼‰"""
        request_params = json.loads(task.request_params)
        symbols = request_params.get('symbols', [])
        
        await self._update_task_total_symbols(task.id, len(symbols))
        logger.info(f"ğŸ¯ å¼€å§‹AIæ¨èä»»åŠ¡: {len(symbols)} åªè‚¡ç¥¨")
        
        results = []
        for i, symbol in enumerate(symbols):
            # æ›´æ–°å½“å‰å¤„ç†çŠ¶æ€
            await self.progress_manager.update_current_symbol(task.id, symbol)
            await self._update_task_current_symbol(task.id, symbol)
            
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_data = self.data_fetcher.get_stock_data(
                symbol, 
                period=request_params.get('period', '1y')
            )
            
            if stock_data is None or stock_data.empty:
                await self.progress_manager.record_symbol_failed(task.id, symbol, "æ•°æ®è·å–å¤±è´¥")
                await self._increment_failed_count(task.id)
                continue
            
            # æµå¼åˆ†æè‚¡ç¥¨
            analysis_result = await self._stream_analyze_symbol(
                task.id, symbol, stock_data, 
                ai_config=json.loads(task.ai_config or '{}'),
                weights=json.loads(task.weights_config or '{}'),
                analysis_logger=analysis_logger
            )
            
            if analysis_result:
                results.append(analysis_result)
                await self.progress_manager.record_symbol_completed(task.id, symbol)
                await self._increment_successful_count(task.id)
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(symbols) * 100
            await self.progress_manager.update_progress(task.id, progress)
            await self._update_task_progress(task.id, progress, i + 1)
        
        # æ’åºå’Œç”Ÿæˆæœ€ç»ˆæ¨è
        final_recommendations = await self._rank_and_select_recommendations(task.id, results)
        await self._save_task_results(task.id, final_recommendations)
        
        return final_recommendations
    
    async def _process_keyword_recommendation(self, task: RecommendationTask, analysis_logger: AnalysisLogger):
        """å¤„ç†å…³é”®è¯æ¨èä»»åŠ¡"""
        request_params = json.loads(task.request_params)
        keyword = request_params.get('keyword')
        max_candidates = request_params.get('max_candidates', 5)
        
        # åœ¨æœ€å¼€å§‹è®°å½•å¼€å§‹æ—¶é—´
        task_start_time = time.time()
        
        # æ›´æ–°ä»»åŠ¡è¯¦æƒ…ä¿¡æ¯
        await self._update_task_details(task, keyword, request_params)
        
        # é˜¶æ®µ1: å…³é”®è¯ç­›é€‰è‚¡ç¥¨
        await self.progress_manager.update_phase(task.id, 'screening')
        analysis_logger.log_ai_screening_start(keyword, max_candidates)
        
        filter_config = json.loads(task.filter_config or '{}')
        # ä½¿ç”¨æ›´å¤§çš„å€™é€‰æ•°è¿›è¡Œåˆæ­¥ç­›é€‰ï¼Œé€šå¸¸æ˜¯æœ€ç»ˆæ¨èæ•°çš„3-4å€
        screening_candidates = max(max_candidates * 3, 20)
        
        candidate_symbols = await self._get_ai_recommended_stocks(
            keyword=keyword,
            max_candidates=screening_candidates,
            exclude_st=filter_config.get('exclude_st', True),
            board=filter_config.get('board'),
            analysis_logger=analysis_logger
        )
        
        logger.info(f"ğŸ“Š ç­›é€‰å‡º {len(candidate_symbols)} åªå€™é€‰è‚¡ç¥¨")
        
        # å­˜å‚¨å€™é€‰è‚¡ç¥¨ä¿¡æ¯
        await self._store_candidate_stocks_info(task.id, candidate_symbols, keyword)
        
        await self._update_task_total_symbols(task.id, len(candidate_symbols))
        await self.progress_manager.update_phase(task.id, 'analyzing')
        
        # è·å–AIé…ç½®å’Œæƒé‡é…ç½®
        ai_config = json.loads(task.ai_config or '{}')
        weights_config = json.loads(task.weights_config or '{}')
        
        # é˜¶æ®µ2: å¹¶è¡Œæµå¼åˆ†æç­›é€‰ç»“æœ
        results = []
        
        # ä¼˜åŒ–ï¼šå°æ‰¹é‡å¹¶è¡Œå¤„ç†ï¼Œé¿å…èµ„æºè€—å°½
        batch_size = min(3, len(candidate_symbols))  # æœ€å¤š3ä¸ªå¹¶è¡Œ
        
        for batch_start in range(0, len(candidate_symbols), batch_size):
            batch_symbols = candidate_symbols[batch_start:batch_start + batch_size]
            
            # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            tasks = []
            for symbol in batch_symbols:
                coroutine_task = self._analyze_single_stock_async(
                    task.id, symbol, request_params, ai_config, weights_config, keyword, analysis_logger
                )
                tasks.append(coroutine_task)
            
            # å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†æ‰¹æ¬¡ç»“æœ
            for i, result in enumerate(batch_results):
                symbol = batch_symbols[i]
                current_index = batch_start + i
                
                if isinstance(result, Exception):
                    logger.error(f"âš ï¸ åˆ†æè‚¡ç¥¨ {symbol} å¤±è´¥: {result}")
                    await self.progress_manager.record_symbol_failed(task.id, symbol, str(result))
                    await self._increment_failed_count(task.id)
                elif result:
                    results.append(result)
                    await self.progress_manager.record_symbol_completed(task.id, symbol)
                    await self._increment_successful_count(task.id)
                else:
                    await self.progress_manager.record_symbol_failed(task.id, symbol, "åˆ†æç»“æœä¸ºç©º")
                    await self._increment_failed_count(task.id)
                
                # æ›´æ–°è¿›åº¦
                progress = (current_index + 1) / len(candidate_symbols) * 100
                await self.progress_manager.update_progress(task.id, progress)
                await self._update_task_progress(task.id, progress, current_index + 1)
        
        # ç”Ÿæˆæœ€ç»ˆæ¨èï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„max_candidates
        final_recommendations = await self._rank_and_select_recommendations(
            task.id,
            results, 
            max_count=max_candidates
        )
        await self._save_task_results(task.id, final_recommendations)
        
        # è®°å½•ä»»åŠ¡å®Œæˆ
        total_duration = time.time() - task_start_time
        if analysis_logger:
            analysis_logger.log_task_complete(
                len(candidate_symbols), len(results), len(final_recommendations), total_duration
            )
        
        logger.info(f"âœ¨ å…³é”®è¯æ¨èå®Œæˆ: {len(final_recommendations)} åªæ¨èè‚¡ç¥¨ (ç”¨æˆ·è®¾ç½®: {max_candidates})")
        
        return final_recommendations
    
    async def _get_ai_recommended_stocks(self, keyword: str, max_candidates: int = 20,
                                       exclude_st: bool = True, board: str = None, 
                                       analysis_logger: AnalysisLogger = None) -> List[str]:
        """ä½¿ç”¨AIè·å–ä¸å…³é”®è¯ç›¸å…³çš„è‚¡ç¥¨æ± ï¼ˆçº¯AIé©±åŠ¨ï¼Œæ— é™çº§ç­–ç•¥ï¼‰"""
        
        try:
            import akshare as ak
            
            logger.info(f"ğŸ” å¼€å§‹AIæ™ºèƒ½ç­›é€‰ä¸ '{keyword}' ç›¸å…³çš„{max_candidates}åªè‚¡ç¥¨")
            
            # è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨
            stock_info = ak.stock_info_a_code_name()
            
            # åŸºæœ¬ç­›é€‰
            filtered_stocks = stock_info.copy()
            
            # æ’é™¤STè‚¡ç¥¨
            if exclude_st:
                filtered_stocks = filtered_stocks[~filtered_stocks['name'].str.contains(r'ST|\*ST|é€€', na=False)]
            
            # æ¿å—ç­›é€‰
            if board and board != 'all':
                if board == 'main':
                    filtered_stocks = filtered_stocks[
                        filtered_stocks['code'].str.startswith(('600', '601', '603', '605', '000', '001', '002'))
                    ]
                elif board == 'gem':
                    filtered_stocks = filtered_stocks[filtered_stocks['code'].str.startswith('300')]
                elif board == 'star':
                    filtered_stocks = filtered_stocks[filtered_stocks['code'].str.startswith('688')]
            
            # æ„å»ºè‚¡ç¥¨å€™é€‰é›†ï¼ˆæ‰©å¤§åˆ°500åªï¼Œæä¾›AIæ›´å¤šé€‰æ‹©ï¼‰
            available_stocks = filtered_stocks.head(500)
            
            # ä¼˜åŒ–çš„AIæç¤ºè¯ - æ›´åŠ ç²¾å‡†å’Œä¸“ä¸š
            stock_options = "\n".join([
                f"{row['code']} {row['name']}" 
                for _, row in available_stocks.iterrows()
            ])
            
            prompt = f"""ä½œä¸ºä¸“ä¸šçš„Aè‚¡æŠ•èµ„åˆ†æå¸ˆï¼Œè¯·æ ¹æ®å…³é”®è¯"{keyword}"ä»ä»¥ä¸‹è‚¡ç¥¨æ± ä¸­ç²¾é€‰å‡º{max_candidates}åªæœ€ç›¸å…³çš„ä¼˜è´¨è‚¡ç¥¨ã€‚

**ç­›é€‰æ ‡å‡†ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š**
1. **æ ¸å¿ƒä¸šåŠ¡åŒ¹é…åº¦**ï¼šå…¬å¸ä¸»è¥ä¸šåŠ¡ä¸å…³é”®è¯çš„ç›´æ¥ç›¸å…³æ€§
2. **è¡Œä¸šåœ°ä½ä¸ç«äº‰ä¼˜åŠ¿**ï¼šåœ¨ç›¸å…³é¢†åŸŸçš„å¸‚åœºåœ°ä½å’ŒæŠ€æœ¯å®åŠ›
3. **æˆé•¿æ½œåŠ›ä¸è¶‹åŠ¿**ï¼šæœªæ¥å‘å±•å‰æ™¯å’Œæˆé•¿æ€§
4. **è´¢åŠ¡å¥åº·åº¦**ï¼šç»è¥ç¨³å®šæ€§å’Œç›ˆåˆ©èƒ½åŠ›
5. **æŠ•èµ„ä»·å€¼**ï¼šå½“å‰ä¼°å€¼æ°´å¹³å’ŒæŠ•èµ„æœºä¼š

**è¾“å‡ºè¦æ±‚ï¼š**
- åªè¿”å›è‚¡ç¥¨ä»£ç ï¼Œæ¯è¡Œä¸€ä¸ª
- ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Š
- ä»£ç æ ¼å¼ä¸º6ä½æ•°å­—ï¼ˆå¦‚ï¼š000001ï¼‰
- è¯·ä»æœ€ç›¸å…³çš„å¼€å§‹æ’åˆ—

**å¯é€‰è‚¡ç¥¨æ± ï¼š**
{stock_options}

**è¯·è¿”å›{max_candidates}ä¸ªè‚¡ç¥¨ä»£ç ï¼š**"""
            
            # è®°å½•AIè¯·æ±‚
            if analysis_logger:
                ai_start_time = analysis_logger.log_ai_screening_request(prompt, 'deepseek')
            
            # ä½¿ç”¨DeepSeekæµå¼è°ƒç”¨
            ai_response = await self._call_deepseek_stream(prompt, analysis_logger)
            
            # è§£æAIè¿”å›çš„è‚¡ç¥¨ä»£ç 
            recommended_codes = self._parse_stock_codes(ai_response, available_stocks, max_candidates)
            
            # è®°å½•AIç­›é€‰ç»“æœ
            if analysis_logger:
                analysis_logger.log_ai_screening_response(
                    ai_response, 'deepseek', ai_start_time if 'ai_start_time' in locals() else time.time(),
                    recommended_codes
                )
            
            if not recommended_codes:
                error_msg = f"AIç­›é€‰å¤±è´¥ï¼šæœªè·å–åˆ°ä¸å…³é”®è¯'{keyword}'ç›¸å…³çš„æœ‰æ•ˆè‚¡ç¥¨ä»£ç "
                logger.error(f"âŒ {error_msg}")
                if analysis_logger:
                    analysis_logger.log_error('ai_screening_empty_result', error_msg)
                raise ValueError(error_msg)
            
            logger.info(f"âœ¨ AIç­›é€‰æˆåŠŸï¼š{len(recommended_codes)}åªä¸'{keyword}'ç›¸å…³çš„è‚¡ç¥¨ -> {recommended_codes}")
            
            return recommended_codes
            
        except Exception as e:
            error_msg = f"AIè‚¡ç¥¨ç­›é€‰å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            if analysis_logger:
                analysis_logger.log_error('ai_screening_error', error_msg, error_details={'keyword': keyword})
            raise Exception(error_msg) from e
    
    async def _call_deepseek_stream(self, prompt: str, analysis_logger: AnalysisLogger = None) -> str:
        """ä½¿ç”¨DeepSeekæµå¼è°ƒç”¨"""
        ai_response = ""
        start_time = time.time()
        
        try:
            logger.info("ğŸ”„ å¼€å§‹æµå¼AIè¯·æ±‚ - Provider: deepseek")
            
            # ä½¿ç”¨DeepSeekæµå¼è°ƒç”¨
            async for chunk in self.ai_router.stream_complete(
                prompt=prompt,
                provider='deepseek',
                model='deepseek-chat',
                temperature=0.1  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ç¡®ä¿ç²¾ç¡®æ€§
            ):
                ai_response += chunk
                
            ai_call_time = time.time() - start_time
            logger.info(f"ğŸ•°ï¸ AIè°ƒç”¨è€—æ—¶: {ai_call_time:.2f}ç§’")
            
            return ai_response
            
        except Exception as e:
            error_msg = f"DeepSeekæµå¼è°ƒç”¨å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg) from e
    
    def _parse_stock_codes(self, ai_response: str, available_stocks, max_candidates: int) -> List[str]:
        """è§£æAIè¿”å›çš„è‚¡ç¥¨ä»£ç """
        recommended_codes = []
        
        if not ai_response.strip():
            return recommended_codes
            
        lines = ai_response.strip().split('\n')
        for line in lines:
            # æ¸…ç†å’ŒéªŒè¯è‚¡ç¥¨ä»£ç 
            code = line.strip()
            # æå–6ä½æ•°å­—ä»£ç 
            import re
            code_match = re.search(r'\b(\d{6})\b', code)
            if code_match:
                code = code_match.group(1)
                # éªŒè¯ä»£ç æ˜¯å¦åœ¨å€™é€‰åˆ—è¡¨ä¸­
                if code in available_stocks['code'].values:
                    if code not in recommended_codes:  # é¿å…é‡å¤
                        recommended_codes.append(code)
                        if len(recommended_codes) >= max_candidates:
                            break
                            
        return recommended_codes

    async def _analyze_single_stock_async(self, task_id: str, symbol: str, request_params: dict, 
                                         ai_config: dict, weights_config: dict, context_keyword: str = None,
                                         analysis_logger: AnalysisLogger = None):
        """å¼‚æ­¥åˆ†æå•ä¸ªè‚¡ç¥¨ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
        try:
            await self.progress_manager.update_current_symbol(task_id, symbol)
            await self._update_task_current_symbol(task_id, symbol)
            
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_data = self.data_fetcher.get_stock_data(
                symbol, 
                period=request_params.get('period', '1y')
            )
            
            if stock_data is None or stock_data.empty:
                logger.warning(f"âš ï¸ è‚¡ç¥¨ {symbol} æ•°æ®è·å–å¤±è´¥")
                return None
            
            # æµå¼åˆ†æè‚¡ç¥¨
            analysis_result = await self._stream_analyze_symbol(
                task_id, symbol, stock_data,
                ai_config=ai_config,
                weights=weights_config,
                context_keyword=context_keyword,
                analysis_logger=analysis_logger
            )
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âš ï¸ å¼‚æ­¥åˆ†æè‚¡ç¥¨ {symbol} å¤±è´¥: {e}")
            raise e
    
    
    async def _process_market_recommendation(self, task: RecommendationTask, analysis_logger: AnalysisLogger):
        """å¤„ç†å…¨å¸‚åœºæ¨èä»»åŠ¡"""
        request_params = json.loads(task.request_params)
        max_candidates = request_params.get('max_candidates', 50)
        
        logger.info(f"ğŸŒ å¼€å§‹å…¨å¸‚åœºæ¨èä»»åŠ¡: æœ€å¤§å€™é€‰æ•° {max_candidates}")
        
        # é˜¶æ®µ1: å…¨å¸‚åœºè‚¡ç¥¨ç­›é€‰
        await self.progress_manager.update_phase(task.id, 'screening')
        
        filter_config = json.loads(task.filter_config or '{}')
        candidate_symbols = await self._screen_market_stocks(
            max_candidates=max_candidates,
            exclude_st=filter_config.get('exclude_st', True),
            min_market_cap=filter_config.get('min_market_cap'),
            board=filter_config.get('board')
        )
        
        logger.info(f"ğŸ“Š å…¨å¸‚åœºç­›é€‰å‡º {len(candidate_symbols)} åªå€™é€‰è‚¡ç¥¨")
        
        await self._update_task_total_symbols(task.id, len(candidate_symbols))
        await self.progress_manager.update_phase(task.id, 'analyzing')
        
        # é˜¶æ®µ2: æµå¼åˆ†æç­›é€‰ç»“æœ
        results = []
        for i, symbol in enumerate(candidate_symbols):
            await self.progress_manager.update_current_symbol(task.id, symbol)
            await self._update_task_current_symbol(task.id, symbol)
            
            stock_data = self.data_fetcher.get_stock_data(
                symbol, 
                period=request_params.get('period', '1y')
            )
            
            if stock_data is not None and not stock_data.empty:
                analysis_result = await self._stream_analyze_symbol(
                    task.id, symbol, stock_data,
                    ai_config=json.loads(task.ai_config or '{}'),
                    weights=json.loads(task.weights_config or '{}')
                )
                
                if analysis_result:
                    results.append(analysis_result)
                    await self.progress_manager.record_symbol_completed(task.id, symbol)
                    await self._increment_successful_count(task.id)
            else:
                await self.progress_manager.record_symbol_failed(task.id, symbol, "æ•°æ®è·å–å¤±è´¥")
                await self._increment_failed_count(task.id)
            
            progress = (i + 1) / len(candidate_symbols) * 100
            await self.progress_manager.update_progress(task.id, progress)
            await self._update_task_progress(task.id, progress, i + 1)
        
        # ç”Ÿæˆæœ€ç»ˆæ¨è
        final_recommendations = await self._rank_and_select_recommendations(
            task.id,
            results, 
            max_count=max_candidates // 4  # å–å‰25%ä½œä¸ºæœ€ç»ˆæ¨è
        )
        await self._save_task_results(task.id, final_recommendations)
        
        logger.info(f"âœ¨ å…¨å¸‚åœºæ¨èå®Œæˆ: {len(final_recommendations)} åªæ¨èè‚¡ç¥¨")
        
        return final_recommendations
    
    async def _screen_market_stocks(self, max_candidates: int = 50, exclude_st: bool = True,
                                   min_market_cap: float = None, board: str = None) -> List[str]:
        """ç­›é€‰å…¨å¸‚åœºè‚¡ç¥¨"""
        try:
            import akshare as ak
            
            # è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨
            stock_info = ak.stock_info_a_code_name()
            
            # åŸºæœ¬ç­›é€‰
            filtered_stocks = stock_info.copy()
            
            # æ’é™¤STè‚¡ç¥¨
            if exclude_st:
                filtered_stocks = filtered_stocks[~filtered_stocks['name'].str.contains(r'ST|\*ST|é€€', na=False)]
            
            # æ¿å—ç­›é€‰
            if board and board != 'all':
                if board == 'main':
                    # ä¸»æ¿ï¼š600å¼€å¤´çš„ä¸Šæµ·ä¸»æ¿ï¼Œ000å¼€å¤´çš„æ·±åœ³ä¸»æ¿
                    filtered_stocks = filtered_stocks[
                        filtered_stocks['code'].str.startswith(('600', '601', '603', '605', '000', '001', '002'))
                    ]
                elif board == 'gem':
                    # åˆ›ä¸šæ¿ï¼š300å¼€å¤´
                    filtered_stocks = filtered_stocks[filtered_stocks['code'].str.startswith('300')]
                elif board == 'star':
                    # ç§‘åˆ›æ¿ï¼š688å¼€å¤´
                    filtered_stocks = filtered_stocks[filtered_stocks['code'].str.startswith('688')]
            
            # éšæœºé‡‡æ ·ï¼ˆä¿è¯å¸‚åœºä»£è¡¨æ€§ï¼‰
            if len(filtered_stocks) > max_candidates:
                # æŒ‰å¸‚å€¼æ’åºï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰ï¼Œå–å‰50%çš„å¤§å¸‚å€¼è‚¡ç¥¨
                # ç„¶åéšæœºé‡‡æ ·
                sampled = filtered_stocks.sample(n=max_candidates, random_state=42)
            else:
                sampled = filtered_stocks
            
            result_symbols = sampled['code'].tolist()
            
            logger.info(f"ğŸŒ å…¨å¸‚åœºç­›é€‰ç»“æœ: {len(result_symbols)} åªè‚¡ç¥¨")
            
            return result_symbols
            
        except Exception as e:
            logger.error(f"âš ï¸ å…¨å¸‚åœºç­›é€‰å¤±è´¥: {e}")
            # é€€åŒ–ç­–ç•¥ï¼šè¿”å›ä¸€äº›ä»£è¡¨æ€§è‚¡ç¥¨
            default_stocks = [
                '000001', '000002', '000858', '000876', '000725',  # æ·±åœ³ä¸»æ¿
                '600000', '600036', '600519', '600887', '600028',  # ä¸Šæµ·ä¸»æ¿
                '300015', '300059', '300124', '300142', '300454',  # åˆ›ä¸šæ¿
                '688008', '688036', '688111', '688169', '688981'   # ç§‘åˆ›æ¿
            ]
            return default_stocks[:max_candidates]
    
    async def _stream_analyze_symbol(self, task_id: str, symbol: str, stock_data: pd.DataFrame, 
                                   ai_config: dict, weights: dict, context_keyword: str = None,
                                   analysis_logger: AnalysisLogger = None):
        """æµå¼åˆ†æå•ä¸ªè‚¡ç¥¨ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰"""
        try:
            # è½¬æ¢åˆ—åä¸ºå…¼å®¹æ ¼å¼
            if not isinstance(stock_data.index, pd.DatetimeIndex):
                stock_data = stock_data.reset_index()
            stock_data.columns = [col.lower() for col in stock_data.columns]
            
            # æŠ€æœ¯åˆ†æ
            if analysis_logger:
                analysis_logger.log_technical_analysis_start(symbol)
                
            technical_analysis = basic_analysis(stock_data)
            if not technical_analysis.get("valid"):
                if analysis_logger:
                    analysis_logger.log_error('technical_analysis_invalid', f'æŠ€æœ¯åˆ†ææ— æ•ˆ: {symbol}')
                return None
                
            # æ£€æŸ¥æŠ€æœ¯åˆ†æ˜¯å¦æœ‰æ•ˆ
            tech_score = technical_analysis.get('score')
            if tech_score is None:
                if analysis_logger:
                    analysis_logger.log_error('technical_score_null', f'æŠ€æœ¯åˆ†ä¸ºç©º: {symbol}')
                logger.warning(f"æŠ€æœ¯åˆ†ä¸ºç©ºï¼Œè·³è¿‡è‚¡ç¥¨ {symbol}")
                return None
            
            if analysis_logger:
                analysis_logger.log_technical_analysis_complete(symbol, technical_analysis)
            
            # æ„å»º AI æç¤ºè¯
            prompt = self._build_analysis_prompt(
                symbol, technical_analysis, stock_data, 
                weights or self.default_weights, context_keyword
            )
            
            # æµå¼AIåˆ†æ
            ai_chunks = []
            ai_analysis = ""
            
            await self.progress_manager.record_ai_analysis_start(task_id, symbol)
            
            # è®°å½•AIåˆ†æå¼€å§‹
            ai_start_time = None
            if analysis_logger:
                ai_start_time = analysis_logger.log_ai_analysis_start(symbol, prompt)
            
            async for chunk in self.ai_router.stream_complete(
                prompt=prompt,
                provider=ai_config.get('provider'),
                temperature=ai_config.get('temperature'),
                api_key=ai_config.get('api_key')
            ):
                ai_chunks.append(chunk)
                ai_analysis += chunk
                
                # å®æ—¶æ¨é€AIå†…å®¹
                await self.progress_manager.push_ai_chunk(
                    task_id, symbol, chunk, ai_analysis
                )
                
                # è®°å½•AIæµå¼æ•°æ®å—
                if analysis_logger:
                    analysis_logger.log_ai_analysis_chunk(symbol, chunk, ai_analysis)
            
            # å¤„ç†AIåˆ†æç»“æœ
            ai_scores = self._extract_ai_scores(ai_analysis)
            
            # è®°å½•AIåˆ†æå®Œæˆ
            if analysis_logger and ai_start_time:
                analysis_logger.log_ai_analysis_complete(
                    symbol, ai_analysis, ai_scores, ai_start_time, 
                    ai_config.get('provider', 'openai')
                )
            
            # è®¡ç®—èåˆåˆ†æ•°ï¼Œå¤„ç†ç©ºå€¼æƒ…å†µ
            fusion_score = self._calculate_fusion_score(
                tech_score,  # ä½¿ç”¨å·²ç»æ£€æŸ¥è¿‡çš„tech_score
                ai_scores.get('confidence', 0),
                weights or self.default_weights
            )
            
            # è®°å½•èåˆè¯„åˆ†è®¡ç®—
            if analysis_logger:
                analysis_logger.log_fusion_score_calculation(
                    symbol, tech_score, ai_scores.get('confidence', 0), 
                    fusion_score, weights or self.default_weights
                )
            
            # å¦‚æœèåˆåˆ†è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¯¥è‚¡ç¥¨
            if fusion_score is None:
                if analysis_logger:
                    analysis_logger.log_error('fusion_score_calculation_failed', f'èåˆåˆ†è®¡ç®—å¤±è´¥: {symbol}')
                logger.warning(f"èåˆåˆ†è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è‚¡ç¥¨ {symbol}")
                return None
            
            # è·å–è‚¡ç¥¨åç§°
            stock_name = self._get_stock_name(symbol)
            
            result = {
                'symbol': symbol,
                'name': stock_name,
                'technical_score': tech_score * 10,  # è½¬æ¢ä¸º0-10èŒƒå›´ç”¨äºæŒä¹…åŒ–
                'ai_score': ai_scores.get('confidence', 0),
                'fusion_score': fusion_score,
                'final_score': fusion_score,
                'action': self._determine_action(fusion_score),
                'ai_analysis': ai_analysis,
                'ai_confidence': ai_scores.get('confidence'),
                'ai_reasoning': ai_scores.get('reasoning'),
                'summary': self._generate_summary(technical_analysis, ai_scores),
                'key_factors': json.dumps(self._extract_key_factors(ai_analysis)),
                'technical_indicators': json.dumps(technical_analysis),
                'current_price': stock_data['close'].iloc[-1] if 'close' in stock_data.columns else None,
                'analyzed_at': now_bj()
            }
            
            await self.progress_manager.record_symbol_analysis_complete(task_id, symbol, result)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æè‚¡ç¥¨ {symbol} å¤±è´¥: {e}")
            if analysis_logger:
                analysis_logger.log_error('stock_analysis_error', str(e), symbol)
            await self.progress_manager.record_symbol_failed(task_id, symbol, str(e))
            return None
    
    async def _update_task_details(self, task: RecommendationTask, keyword: str, request_params: dict):
        """æ›´æ–°ä»»åŠ¡è¯¦æƒ…ä¿¡æ¯"""
        with SessionLocal() as db:
            try:
                db_task = db.query(RecommendationTask).filter(RecommendationTask.id == task.id).first()
                if db_task:
                    # ç”¨æˆ·è¾“å…¥æ‘˜è¦
                    user_summary_parts = []
                    if task.task_type == 'keyword':
                        user_summary_parts.append(f"å…³é”®è¯: {keyword}")
                    elif task.task_type == 'market':
                        user_summary_parts.append("å…¨å¸‚åœºæ¨è")
                    
                    max_candidates = request_params.get('max_candidates', 'æœªè®¾ç½®')
                    user_summary_parts.append(f"æœ€å¤§å€™é€‰æ•°: {max_candidates}")
                    
                    period = request_params.get('period', '1y')
                    user_summary_parts.append(f"åˆ†æå‘¨æœŸ: {period}")
                    
                    db_task.user_input_summary = " | ".join(user_summary_parts)
                    
                    # ç­›é€‰æ¡ä»¶æ‘˜è¦
                    filter_config = json.loads(task.filter_config or '{}')
                    filter_summary_parts = []
                    if filter_config.get('exclude_st'):
                        filter_summary_parts.append("æ’é™¤STè‚¡ç¥¨")
                    
                    board = filter_config.get('board')
                    if board and board != 'all':
                        board_names = {'main': 'ä¸»æ¿', 'gem': 'åˆ›ä¸šæ¿', 'star': 'ç§‘åˆ›æ¿'}
                        filter_summary_parts.append(f"æ¿å—: {board_names.get(board, board)}")
                    
                    min_market_cap = filter_config.get('min_market_cap')
                    if min_market_cap:
                        filter_summary_parts.append(f"æœ€å°å¸‚å€¼: {min_market_cap}äº¿")
                    
                    db_task.filter_summary = " | ".join(filter_summary_parts) if filter_summary_parts else "æ— ç‰¹æ®Šç­›é€‰æ¡ä»¶"
                    
                    # æ‰§è¡Œç­–ç•¥è¯´æ˜
                    if task.task_type == 'keyword':
                        db_task.execution_strategy = "AIæ™ºèƒ½ç­›é€‰è‚¡ç¥¨æ±  â†’ æŠ€æœ¯åˆ†æ â†’ AIæ·±åº¦åˆ†æ â†’ èåˆè¯„åˆ† â†’ æ’åºæ¨è"
                    else:
                        db_task.execution_strategy = "å¸‚åœºéšæœºé‡‡æ · â†’ æŠ€æœ¯åˆ†æ â†’ AIæ·±åº¦åˆ†æ â†’ èåˆè¯„åˆ† â†’ æ’åºæ¨è"
                    
                    db.commit()
                    logger.info(f"âœ… ä»»åŠ¡ {task.id} è¯¦æƒ…ä¿¡æ¯å·²æ›´æ–°")
                    
            except Exception as e:
                db.rollback()
                logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")

    async def _store_candidate_stocks_info(self, task_id: str, candidate_symbols: List[str], keyword: str = None):
        """å­˜å‚¨å€™é€‰è‚¡ç¥¨ä¿¡æ¯"""
        with SessionLocal() as db:
            try:
                db_task = db.query(RecommendationTask).filter(RecommendationTask.id == task_id).first()
                if db_task:
                    # è·å–è‚¡ç¥¨åç§°ä¿¡æ¯
                    import akshare as ak
                    try:
                        stock_info = ak.stock_info_a_code_name()
                        code_name_map = dict(zip(stock_info['code'], stock_info['name']))
                    except:
                        code_name_map = {}
                    
                    # æ„å»ºå€™é€‰è‚¡ç¥¨ä¿¡æ¯
                    candidate_info = []
                    for i, symbol in enumerate(candidate_symbols, 1):
                        stock_name = code_name_map.get(symbol, f"è‚¡ç¥¨{symbol}")
                        candidate_info.append({
                            "rank": i,
                            "code": symbol,
                            "name": stock_name,
                            "selected_by": "AIæ¨è" if keyword else "å¸‚åœºç­›é€‰"
                        })
                    
                    # å­˜å‚¨ä¸ºJSONæ ¼å¼
                    candidate_summary = {
                        "total_count": len(candidate_symbols),
                        "selection_method": f"AIå…³é”®è¯ç­›é€‰: {keyword}" if keyword else "å…¨å¸‚åœºéšæœºé‡‡æ ·",
                        "candidates": candidate_info
                    }
                    
                    db_task.candidate_stocks_info = json.dumps(candidate_summary, ensure_ascii=False, indent=2)
                    db.commit()
                    logger.info(f"âœ… å€™é€‰è‚¡ç¥¨ä¿¡æ¯å·²å­˜å‚¨åˆ°ä»»åŠ¡ {task_id}")
                    
            except Exception as e:
                db.rollback()
                logger.error(f"âŒ å­˜å‚¨å€™é€‰è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
    
    async def _process_watchlist_batch(self, task: RecommendationTask, analysis_logger: AnalysisLogger):
        """å¤„ç†è‡ªé€‰è‚¡ç¥¨æ‰¹é‡åˆ†æä»»åŠ¡"""
        request_params = json.loads(task.request_params)
        symbols = request_params.get('symbols')
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œè·å–æ‰€æœ‰è‡ªé€‰è‚¡
        if not symbols:
            symbols = await self._get_all_watchlist_symbols()
            if not symbols:
                error_msg = "è‡ªé€‰è‚¡åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œæ‰¹é‡åˆ†æ"
                logger.error(f"âŒ {error_msg}")
                analysis_logger.log_error('empty_watchlist', error_msg)
                raise ValueError(error_msg)
        
        await self._update_task_total_symbols(task.id, len(symbols))
        logger.info(f"ğŸ¯ å¼€å§‹è‡ªé€‰è‚¡ç¥¨æ‰¹é‡åˆ†æä»»åŠ¡: {len(symbols)} åªè‚¡ç¥¨")
        
        # è®°å½•ä»»åŠ¡è¯¦æƒ…
        await self._update_watchlist_task_details(task, symbols, 'batch')
        
        results = []
        for i, symbol in enumerate(symbols):
            # æ›´æ–°å½“å‰å¤„ç†çŠ¶æ€
            await self.progress_manager.update_current_symbol(task.id, symbol)
            await self._update_task_current_symbol(task.id, symbol)
            
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_data = self.data_fetcher.get_stock_data(
                symbol, 
                period=request_params.get('period', '1y')
            )
            
            if stock_data is None or stock_data.empty:
                await self.progress_manager.record_symbol_failed(task.id, symbol, "æ•°æ®è·å–å¤±è´¥")
                await self._increment_failed_count(task.id)
                continue
            
            # æµå¼åˆ†æè‚¡ç¥¨
            analysis_result = await self._stream_analyze_symbol(
                task.id, symbol, stock_data, 
                ai_config=json.loads(task.ai_config or '{}'),
                weights=json.loads(task.weights_config or '{}'),
                analysis_logger=analysis_logger
            )
            
            if analysis_result:
                results.append(analysis_result)
                await self.progress_manager.record_symbol_completed(task.id, symbol)
                await self._increment_successful_count(task.id)
                
                # ä¿å­˜åˆ†æç»“æœåˆ°è‡ªé€‰è‚¡åˆ†æè®°å½•
                await self._save_watchlist_analysis_result(symbol, analysis_result)
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(symbols) * 100
            await self.progress_manager.update_progress(task.id, progress)
            await self._update_task_progress(task.id, progress, i + 1)
        
        # æ’åºå’Œç”Ÿæˆæœ€ç»ˆæ¨è
        final_recommendations = await self._rank_and_select_recommendations(task.id, results)
        await self._save_task_results(task.id, final_recommendations)
        
        logger.info(f"âœ¨ è‡ªé€‰è‚¡ç¥¨æ‰¹é‡åˆ†æå®Œæˆ: {len(final_recommendations)} åªè‚¡ç¥¨")
        
        return final_recommendations
    
    async def _process_watchlist_reanalyze(self, task: RecommendationTask, analysis_logger: AnalysisLogger):
        """å¤„ç†è‡ªé€‰è‚¡ç¥¨å•è‚¡é‡æ–°åˆ†æä»»åŠ¡"""
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†è‡ªé€‰è‚¡é‡æ–°åˆ†æä»»åŠ¡: {task.id}")
        request_params = json.loads(task.request_params)
        symbol = request_params.get('symbol')
        logger.info(f"ğŸ“Š è§£æè¯·æ±‚å‚æ•° - è‚¡ç¥¨ä»£ç : {symbol}, å‘¨æœŸ: {request_params.get('period', '1y')}")
        
        if not symbol:
            error_msg = "è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º"
            logger.error(f"âŒ {error_msg}")
            analysis_logger.log_error('missing_symbol', error_msg)
            raise ValueError(error_msg)
        
        # éªŒè¯è‚¡ç¥¨æ˜¯å¦åœ¨è‡ªé€‰è‚¡åˆ—è¡¨ä¸­
        logger.info(f"ğŸ” éªŒè¯è‚¡ç¥¨ {symbol} æ˜¯å¦åœ¨è‡ªé€‰è‚¡åˆ—è¡¨ä¸­...")
        watchlist_symbols = await self._get_all_watchlist_symbols()
        logger.info(f"ğŸ“‹ è‡ªé€‰è‚¡åˆ—è¡¨: {watchlist_symbols}")
        
        if symbol not in watchlist_symbols:
            error_msg = f"è‚¡ç¥¨ {symbol} ä¸åœ¨è‡ªé€‰è‚¡åˆ—è¡¨ä¸­ï¼Œå½“å‰è‡ªé€‰è‚¡: {watchlist_symbols}"
            logger.error(f"âŒ {error_msg}")
            analysis_logger.log_error('symbol_not_in_watchlist', error_msg)
            raise ValueError(error_msg)
        
        logger.info(f"âœ… è‚¡ç¥¨ {symbol} éªŒè¯é€šè¿‡ï¼Œå¼€å§‹åˆ†æ...")
        
        await self._update_task_total_symbols(task.id, 1)
        logger.info(f"ğŸ¯ å¼€å§‹è‡ªé€‰è‚¡ç¥¨é‡æ–°åˆ†æä»»åŠ¡: {symbol}")
        
        # è®°å½•ä»»åŠ¡è¯¦æƒ…
        await self._update_watchlist_task_details(task, [symbol], 'reanalyze')
        
        # æ›´æ–°å½“å‰å¤„ç†çŠ¶æ€
        await self.progress_manager.update_current_symbol(task.id, symbol)
        await self._update_task_current_symbol(task.id, symbol)
        
        # è·å–è‚¡ç¥¨æ•°æ®
        stock_data = self.data_fetcher.get_stock_data(
            symbol, 
            period=request_params.get('period', '1y')
        )
        
        if stock_data is None or stock_data.empty:
            error_msg = f"è‚¡ç¥¨ {symbol} æ•°æ®è·å–å¤±è´¥"
            await self.progress_manager.record_symbol_failed(task.id, symbol, error_msg)
            await self._increment_failed_count(task.id)
            analysis_logger.log_error('data_fetch_failed', error_msg)
            raise ValueError(error_msg)
        
        # æµå¼åˆ†æè‚¡ç¥¨
        analysis_result = await self._stream_analyze_symbol(
            task.id, symbol, stock_data, 
            ai_config=json.loads(task.ai_config or '{}'),
            weights=json.loads(task.weights_config or '{}'),
            analysis_logger=analysis_logger
        )
        
        if analysis_result:
            await self.progress_manager.record_symbol_completed(task.id, symbol)
            await self._increment_successful_count(task.id)
            
            # ä¿å­˜åˆ†æç»“æœåˆ°è‡ªé€‰è‚¡åˆ†æè®°å½•
            await self._save_watchlist_analysis_result(symbol, analysis_result)
            
            # æ›´æ–°è¿›åº¦ä¸º100%
            await self.progress_manager.update_progress(task.id, 100.0)
            await self._update_task_progress(task.id, 100.0, 1)
            
            # ä¿å­˜ä»»åŠ¡ç»“æœ
            await self._save_task_results(task.id, [analysis_result])
            
            logger.info(f"âœ¨ è‡ªé€‰è‚¡ç¥¨é‡æ–°åˆ†æå®Œæˆ: {symbol}")
            
            return [analysis_result]
        else:
            error_msg = f"è‚¡ç¥¨ {symbol} åˆ†æå¤±è´¥"
            await self.progress_manager.record_symbol_failed(task.id, symbol, error_msg)
            await self._increment_failed_count(task.id)
            analysis_logger.log_error('analysis_failed', error_msg)
            raise ValueError(error_msg)
    
    async def _get_all_watchlist_symbols(self) -> List[str]:
        """è·å–æ‰€æœ‰è‡ªé€‰è‚¡ç¥¨ä»£ç """
        try:
            logger.info("ğŸ” å¼€å§‹è·å–è‡ªé€‰è‚¡åˆ—è¡¨...")
            # å¯¼å…¥è‡ªé€‰è‚¡ç›¸å…³æ¨¡å‹ï¼ˆä»routes.pyå¯¼å…¥ï¼‰
            from backend.routes import Watchlist, SessionLocal as WatchlistSessionLocal
            logger.info("âœ… æˆåŠŸå¯¼å…¥Watchlistæ¨¡å‹å’ŒSessionLocal")
            
            with WatchlistSessionLocal() as db:
                logger.info("ğŸ”— æ•°æ®åº“è¿æ¥å·²å»ºç«‹")
                watchlist_items = db.query(Watchlist).all()
                logger.info(f"ğŸ“Š æŸ¥è¯¢åˆ° {len(watchlist_items)} æ¡è‡ªé€‰è‚¡è®°å½•")
                
                symbols = [item.symbol for item in watchlist_items]
                logger.info(f"ğŸ“‹ è·å–è‡ªé€‰è‚¡åˆ—è¡¨: {len(symbols)} åªè‚¡ç¥¨ - {symbols}")
                return symbols
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‡ªé€‰è‚¡åˆ—è¡¨å¤±è´¥: {e}")
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []
    
    async def _save_watchlist_analysis_result(self, symbol: str, analysis_result: dict):
        """ä¿å­˜è‡ªé€‰è‚¡åˆ†æç»“æœåˆ°AnalysisRecordè¡¨"""
        try:
            # å¯¼å…¥åˆ†æè®°å½•æ¨¡å‹ï¼ˆä»routes.pyå¯¼å…¥ï¼‰
            from backend.routes import AnalysisRecord, SessionLocal as WatchlistSessionLocal
            
            with WatchlistSessionLocal() as db:
                # åˆ›å»ºæ–°çš„åˆ†æè®°å½•
                analysis_record = AnalysisRecord(
                    symbol=symbol,
                    score=analysis_result.get('final_score'),  # ä¿®å¤ï¼šä½¿ç”¨final_scoreä½œä¸ºscoreå­—æ®µ
                    action=analysis_result.get('action'),
                    reason_brief=analysis_result.get('summary'),  # ä¿®å¤ï¼šä½¿ç”¨summaryä½œä¸ºreason_brief
                    ai_advice=analysis_result.get('ai_analysis'),  # ä¿®å¤ï¼šä½¿ç”¨ai_analysisä½œä¸ºai_advice
                    ai_confidence=analysis_result.get('ai_confidence'),
                    fusion_score=analysis_result.get('fusion_score'),
                    created_at=analysis_result.get('analyzed_at', now_bj())  # ä¿®å¤ï¼šä½¿ç”¨created_atè€Œä¸æ˜¯analyzed_at
                )
                
                db.add(analysis_record)
                db.commit()
                
                logger.info(f"âœ… è‡ªé€‰è‚¡ {symbol} åˆ†æç»“æœå·²ä¿å­˜åˆ°AnalysisRecord")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è‡ªé€‰è‚¡ {symbol} åˆ†æç»“æœå¤±è´¥: {e}")
    
    async def _update_watchlist_task_details(self, task: RecommendationTask, symbols: List[str], task_subtype: str):
        """æ›´æ–°è‡ªé€‰è‚¡ä»»åŠ¡è¯¦æƒ…ä¿¡æ¯"""
        with SessionLocal() as db:
            try:
                db_task = db.query(RecommendationTask).filter(RecommendationTask.id == task.id).first()
                if db_task:
                    # ç”¨æˆ·è¾“å…¥æ‘˜è¦
                    if task_subtype == 'batch':
                        if len(symbols) == await self._get_watchlist_total_count():
                            db_task.user_input_summary = f"è‡ªé€‰è‚¡æ‰¹é‡åˆ†æ | å…¨éƒ¨è‡ªé€‰è‚¡ ({len(symbols)}åª)"
                        else:
                            db_task.user_input_summary = f"è‡ªé€‰è‚¡æ‰¹é‡åˆ†æ | æŒ‡å®šè‚¡ç¥¨ ({len(symbols)}åª): {', '.join(symbols[:5])}{'...' if len(symbols) > 5 else ''}"
                    else:  # reanalyze
                        db_task.user_input_summary = f"è‡ªé€‰è‚¡é‡æ–°åˆ†æ | è‚¡ç¥¨ä»£ç : {symbols[0]}"
                    
                    # ç­›é€‰æ¡ä»¶æ‘˜è¦
                    db_task.filter_summary = "è‡ªé€‰è‚¡ç¥¨æ±  | æ— é¢å¤–ç­›é€‰æ¡ä»¶"
                    
                    # æ‰§è¡Œç­–ç•¥è¯´æ˜
                    if task_subtype == 'batch':
                        db_task.execution_strategy = "è·å–è‡ªé€‰è‚¡åˆ—è¡¨ â†’ æŠ€æœ¯åˆ†æ â†’ AIæ·±åº¦åˆ†æ â†’ èåˆè¯„åˆ† â†’ æ›´æ–°åˆ†æè®°å½•"
                    else:
                        db_task.execution_strategy = "éªŒè¯è‡ªé€‰è‚¡ â†’ æŠ€æœ¯åˆ†æ â†’ AIæ·±åº¦åˆ†æ â†’ èåˆè¯„åˆ† â†’ æ›´æ–°åˆ†æè®°å½•"
                    
                    db.commit()
                    logger.info(f"âœ… è‡ªé€‰è‚¡ä»»åŠ¡ {task.id} è¯¦æƒ…ä¿¡æ¯å·²æ›´æ–°")
                    
            except Exception as e:
                db.rollback()
                logger.error(f"âŒ æ›´æ–°è‡ªé€‰è‚¡ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
    
    async def _get_watchlist_total_count(self) -> int:
        """è·å–è‡ªé€‰è‚¡æ€»æ•°"""
        try:
            # å¯¼å…¥è‡ªé€‰è‚¡ç›¸å…³æ¨¡å‹ï¼ˆä»routes.pyå¯¼å…¥ï¼‰
            from backend.routes import Watchlist, SessionLocal as WatchlistSessionLocal
            
            with WatchlistSessionLocal() as db:
                count = db.query(Watchlist).count()
                return count
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‡ªé€‰è‚¡æ€»æ•°å¤±è´¥: {e}")
            return 0
